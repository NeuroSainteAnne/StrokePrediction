{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYbx9hT-w2BA"
   },
   "source": [
    "## Module loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4eoO16Yew2BG"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import nibabel as nib\n",
    "import random\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from natsort import natsorted\n",
    "from collections import Counter\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import mixed_precision\n",
    "import progressbar\n",
    "from modules.generator import DataGenerator\n",
    "from modules.model import Unet2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGZT5pbFw2BI"
   },
   "source": [
    "## Data loading\n",
    "\n",
    "The data must be in the following format :\n",
    "- one **metadata.hdf5** file containing the following variables :\n",
    "    - *\"patientnames\"*, a list with all patient identifiers\n",
    "    - *\"shape_x\"*, the numpy shape of the X array - typically, (n, 256, 256, 27, 3)\n",
    "    - *\"shape_y\"*, the numpy shape of the Y array - typically, (n, 256, 256, 27, 1)\n",
    "    - *\"shape_mask\"*, the numpy shape of the Brain mask array - typically, (n, 256, 256, 27, 1)\n",
    "- Four **data_?.dat** files consisting in numpy memmaps\n",
    "    - *\"data_x.dat\"* in float32 with the following sequences stored in this order: \n",
    "        - H0 DWI b1000 (normalized with centered mean and divided by standard deviation)\n",
    "        - ADC (in .10-6 mm2/sec)\n",
    "        - TMax map (in seconds)\n",
    "    - *\"data_y.dat\"* in float32 with H24 stroke segmentations (binary)\n",
    "    - *\"data_mask.dat\"* in uint8 with the brain weighting sequence\n",
    "        - value = 0 for out-of-brain voxels\n",
    "        - value = 1 for in-brain voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qRt0aQFw2BJ"
   },
   "outputs": [],
   "source": [
    "sourcedir = \"data/\" # Data directory\n",
    "model_path = \"models/\" # output directory\n",
    "\n",
    "with h5py.File(os.path.join(sourcedir,\"metadata.hdf5\"), \"r\") as data:\n",
    "    train_names = [l.decode() for l in list(data[\"patientnames\"])]\n",
    "    shape_x = tuple(data[\"shape_x\"])\n",
    "    shape_y = tuple(data[\"shape_y\"])\n",
    "    shape_mask = tuple(data[\"shape_mask\"])\n",
    "    \n",
    "datax = np.memmap(os.path.join(sourcedir, \"data_x.dat\"), dtype=\"float32\", mode=\"r\", shape=shape_x)\n",
    "datay = np.memmap(os.path.join(sourcedir, \"data_y.dat\"), dtype=\"float32\", mode=\"r\", shape=shape_y)\n",
    "datamask = np.memmap(os.path.join(sourcedir, \"data_mask.dat\"), dtype=\"uint8\", mode=\"r\", shape=shape_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "algnRIsjw2BK"
   },
   "source": [
    "## Data splitting\n",
    "\n",
    "Data is split between train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NicK4Z2w2BL"
   },
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Klj9shRw2BM"
   },
   "outputs": [],
   "source": [
    "train_index, test_index = train_test_split(range(len(train_names)), \n",
    "                                           test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "print(\"Stratification count\")\n",
    "print(\"Training set: \", Counter([train_names[i] for i in train_index]))\n",
    "print(\"Test set: \", Counter([train_names[i] for i in test_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTNxj64yw2BN",
    "tags": []
   },
   "source": [
    "## Showing erratic data\n",
    "Looks up for volumes containing DWI voxel values <-5 or >12 and shows the middle slice.\n",
    "\n",
    "Please check the corresponding volumes of these patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHw5-D75w2BN"
   },
   "outputs": [],
   "source": [
    "flatmax = datax[...,0].max(axis=(1,2,3))\n",
    "flatmin = datax[...,0].min(axis=(1,2,3))\n",
    "erratic = np.where(np.logical_or(flatmax>12,flatmin<-5))[0]\n",
    "if len(erratic) > 0:\n",
    "    plt.rcParams['figure.figsize'] = [15, 5]\n",
    "    print([train_names[i] for i in erratic])\n",
    "    for i in range(len(erratic)):\n",
    "        j = erratic[i]\n",
    "        plt.subplot(1,len(erratic),i+1)\n",
    "        plt.imshow(np.flipud(datay[j,:,:,16,0].T), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nE7GVSGhw2BO"
   },
   "source": [
    "## Checking data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e_OCGh-w2BQ"
   },
   "outputs": [],
   "source": [
    "check_generator = DataGenerator(datax=datax,\n",
    "                                datay=datay,\n",
    "                                mask=datamask,\n",
    "                                indices=np.arange(len(train_names)),\n",
    "                                shuffle=True, \n",
    "                                flatten_output=False,\n",
    "                                batch_size=1, dim_z=1,\n",
    "                                augment=True, flipaugm=True, brightaugm=[True,True,False], gpu_augment=True,\n",
    "                                scale_input=True, scale_input_lim=[(-5,12),(0,7500.0),(-30,120)], scale_input_clip=[True,True,True],\n",
    "                                only_stroke=True, give_mask=True)\n",
    "\n",
    "check_gen_iter = check_generator.getnext()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "n_row = 4\n",
    "for i in range(n_row):\n",
    "    sampleX, sampleY = next(check_gen_iter)\n",
    "    plt.subplot(n_row,5,i*5+1)\n",
    "    plt.title('Diffusion imaging')\n",
    "    plt.imshow(np.flipud(sampleX[\"img\"][:,:,0,0].T), cmap='gray', vmin=-0.8, vmax=1)\n",
    "    plt.subplot(n_row,5,i*5+2)\n",
    "    plt.title('ADC')\n",
    "    plt.imshow(np.flipud(sampleX[\"img\"][:,:,0,1].T), cmap='gray', vmin=-0.8, vmax=1)\n",
    "    plt.subplot(n_row,5,i*5+3)\n",
    "    plt.title('TMax')\n",
    "    plt.imshow(np.flipud(sampleX[\"img\"][:,:,0,2].T), cmap='gray', vmin=-1.2, vmax=1)\n",
    "    plt.subplot(n_row,5,i*5+4)\n",
    "    plt.title('Mask')\n",
    "    plt.imshow(np.flipud(sampleX[\"mask\"][:,:,0].T), cmap='gray', vmin=0, vmax=1)\n",
    "    plt.subplot(n_row,5,i*5+5)\n",
    "    plt.title('Final stroke segmentation')\n",
    "    plt.imshow(np.flipud(sampleY[:,:,0].T), cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwJW6Mm5w2BQ"
   },
   "source": [
    "## Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cs7iDbGnw2BQ"
   },
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(datax, datay, indices=train_index, dim_z=3, batch_size=1, augment=False, \n",
    "                                scale_lim=[(-5,12),(0,7500.0),(-30,120)], brightaugm=False)\n",
    "dsT = tf.data.Dataset.from_generator(train_generator.getnext, ({\"img\":K.floatx()}, K.floatx()), \n",
    "                                    ({\"img\":(256,256,3,3)}, (256*256,))).repeat().batch(batch_size).prefetch(16)\n",
    "\n",
    "batch_size = 16\n",
    "input_img = Input((256,256,3,3), name='img')\n",
    "model = UNet2D(input_img, n_filters=256, dropout=0.5, batchnorm=True)\n",
    "\n",
    "model.compile(optimizer=Adam(0.003), loss=\"binary_crossentropy\")\n",
    "model.fit(dsT, epochs=20, steps_per_epoch=len(train_generator)//batch_size, callbacks=[\n",
    "        ModelCheckpoint(\"output/checkpoints/checkpoint\", verbose=1, save_weights_only=True),\n",
    "        TensorBoard(log_dir=\"output/tf_logs/\"+lastcheckpoint+\"/\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "synthFLAIR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
